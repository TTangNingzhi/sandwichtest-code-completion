{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "DcU84YZbsLcEJAKMWHpirs",
     "type": "MD"
    }
   },
   "source": [
    "# Evaluation of code completion with ðŸ¤— and vLLM\n",
    "\n",
    "In this notebook, we provide an example of how you can use vLLM and open-source models to set up fast evaluation similar to what we do in the competition. The notebook closely follows what we do in the competition evaluator, with the major difference being that we use cloud inference in the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "VhEaAExAKFzMyhv9m6W68d",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ntang/miniconda3/envs/jb-completion/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-23 13:41:02 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import jsonlines\n",
    "from dataclasses import dataclass\n",
    "from typing import Callable\n",
    "\n",
    "# We will use vLLM for fast inference\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "from codegen_metrics import chrf\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "DATA_DIR = os.getenv(\"DATA_DIR\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Mn6G8nB65UedC8mbZJHXlU",
     "type": "MD"
    }
   },
   "source": [
    "## Special tokens\n",
    "The listed special tokens are used by the evaluated models to construct correct sequences for FIM (fill in the middle).\n",
    "We suggest using them for your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "u37rtBMXLR8UZDILxKgKrG",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelTokens:\n",
    "    filename: str\n",
    "    prefix: str\n",
    "    suffix: str\n",
    "    middle: str\n",
    "\n",
    "    def special_tokens(self):\n",
    "        return [self.filename, self.prefix, self.suffix, self.middle]\n",
    "\n",
    "qwen_tokens = ModelTokens(\"<|file_sep|>\", \"<|fim_prefix|>\", \"<|fim_suffix|>\", \"<|fim_middle|>\")\n",
    "mellum_tokens = ModelTokens(\"<filename>\", \"<fim_prefix>\", \"<fim_suffix>\", \"<fim_middle>\")\n",
    "codestral_tokens = ModelTokens(\"+++++\", \"[PREFIX]\", \"[SUFFIX]\", \"[MIDDLE]\")\n",
    "\n",
    "DEFAULT_FILE_SEP = qwen_tokens.filename"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Fzv7QSTSKYgTekL57nKNek",
     "type": "MD"
    }
   },
   "source": [
    "## Model inference\n",
    "\n",
    "In this example, we use vLLM for fast inference and use sampling parameters for greedy decoding. In the competition evaluator, we rely on a separate remote provider for each model and use similar sampling parameters to ensure stable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "UkADsbeXNJj9kKKAMZI2cI",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "class VLLMSampler:\n",
    "    def __init__(\n",
    "            self, \n",
    "            model_name: str, \n",
    "            special_tokens: list[str],\n",
    "            sampling_parameters: dict,\n",
    "    ):\n",
    "        self.sampling_params = SamplingParams(stop=special_tokens, **sampling_parameters)\n",
    "        self.model_name = model_name\n",
    "        self.model = LLM(model=model_name)\n",
    "\n",
    "    def generate(self, prompts):\n",
    "        outputs = self.model.generate(prompts, self.sampling_params)\n",
    "        completions = [output.outputs[0].text for output in outputs]\n",
    "        return completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Oj0o4BVCDwucfyHqq0e6kB",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "sampling_params = {\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_tokens\": 384,\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "V7D6XMQrQryou6MbSdtx5a",
     "type": "MD"
    }
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "For correct construction of FIM prompt, please refer to the docs of the respective model ([Mellum](https://huggingface.co/JetBrains/Mellum-4b-sft-python#fill-in-the-middle-with-additional-files-as-context-generation), [Qwen](https://github.com/QwenLM/Qwen2.5-Coder?tab=readme-ov-file#-code-with-qwen25-coder-32b), [Codestral](https://huggingface.co/mistralai/Codestral-22B-v0.1#fill-in-the-middle-fim)). Here we use the format provided by Mellum authors, as per [the model page](https://huggingface.co/JetBrains/Mellum-4b-sft-python).\n",
    "\n",
    "We trim the context by lines to fit into the context window. We remove the starting lines first, so we assume that the most relevant context appears closer to the end of the context sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "eVVewxiCyuUdww2V7tnw0X",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# We assume that the .jsonl file with the dataset is in the \"data/\" folder\n",
    "def read_dataset(dataset_name: str, folder_path: str) -> list[dict]:\n",
    "    with jsonlines.open(os.path.join(folder_path, f\"{dataset_name}.jsonl\"), \"r\") as f:\n",
    "        dataset = [datapoint for datapoint in f]\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "L6QChZz4SAu39SkvUnGnsA",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def truncate_context(context: str, prefix: str, suffix: str, max_new_tokens: int, max_tokens: int, encode: Callable) -> str:\n",
    "    num_tokens = max_new_tokens\n",
    "    num_tokens += len(encode(prefix))\n",
    "    num_tokens += len(encode(suffix))\n",
    "\n",
    "    context_lines = context.splitlines(keepends=True)\n",
    "    truncated_context = ''\n",
    "    while len(context_lines) > 0:\n",
    "        # Adding lines from the end of the provided context\n",
    "        curr_line = context_lines.pop(-1)\n",
    "        line_tokens = len(encode(curr_line))\n",
    "        num_tokens += line_tokens\n",
    "        if num_tokens > max_tokens:\n",
    "            break\n",
    "        else:\n",
    "            truncated_context = curr_line + truncated_context\n",
    "\n",
    "    return truncated_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "wc6orhm6b8GgY260zMpKTl",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Python version of Mellum\n",
    "mellum_tokenizer = AutoTokenizer.from_pretrained(\"JetBrains/Mellum-4b-sft-python\")\n",
    "\n",
    "def build_mellum_prompt(context: str, filename: str, prefix: str, suffix: str):\n",
    "    context.replace(DEFAULT_FILE_SEP, mellum_tokens.filename)\n",
    "    truncated_context = truncate_context(\n",
    "        context, prefix, suffix, \n",
    "        max_new_tokens=sampling_params[\"max_tokens\"], max_tokens=8000, encode=mellum_tokenizer.tokenize\n",
    "    )\n",
    "    context = f\"{truncated_context}\\n\"\n",
    "    filename = f\"{mellum_tokens.filename}{filename}\\n\"\n",
    "    prefix = f\"{mellum_tokens.prefix}{prefix}\"\n",
    "    suffix = f\"{mellum_tokens.suffix}{suffix}\"\n",
    "    middle = f\"{mellum_tokens.middle}\"\n",
    "    prompt = context + filename + suffix + prefix + middle\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "N185WrZQfLGiLf0ScTMRY9",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "dataset = read_dataset(\"python-public\", DATA_DIR)\n",
    "answers = read_dataset(\"answers-python-public\", DATA_DIR)\n",
    "contexts = read_dataset(\"python-public-ningzhi\", \"../predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "vilQbQt4tMQ7Ye7qC7gFm2",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "mellum_prompts = []\n",
    "\n",
    "for datapoint, prediction in zip(dataset, contexts):\n",
    "    context = prediction[\"context\"]\n",
    "    filename = datapoint[\"path\"]\n",
    "    prefix = prediction.get(\"prefix\", datapoint[\"prefix\"])\n",
    "    suffix = prediction.get(\"suffix\", datapoint[\"suffix\"])\n",
    "    prompt = build_mellum_prompt(context, filename, prefix, suffix)\n",
    "    mellum_prompts.append(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "nnrRQM5rsWSOWrkD5XQtS4",
     "type": "MD"
    }
   },
   "source": [
    "## Evaluation example\n",
    "\n",
    "As an example, we provide inference of the open-source version of Mellum fine-tuned on Python. For Kotlin, please use the respective [fine-tuned version](https://huggingface.co/JetBrains/Mellum-4b-sft-kotlin).\n",
    "\n",
    "We use ChrF as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "rVeN4xS16pNvQOBOVQu4Mo",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-23 13:41:11 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 07-23 13:41:11 [config.py:1472] Using max model len 8192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-23 13:41:11,533\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-23 13:41:11 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 07-23 13:41:11 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 07-23 13:41:11 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='JetBrains/Mellum-4b-sft-python', speculative_config=None, tokenizer='JetBrains/Mellum-4b-sft-python', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=JetBrains/Mellum-4b-sft-python, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-23 13:41:12 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 07-23 13:41:12 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-23 13:41:12 [gpu_model_runner.py:1770] Starting to load model JetBrains/Mellum-4b-sft-python...\n",
      "INFO 07-23 13:41:12 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 07-23 13:41:12 [cuda.py:284] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-23 13:41:13 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.93it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.17it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.13it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-23 13:41:14 [default_loader.py:272] Loading weights took 1.00 seconds\n",
      "INFO 07-23 13:41:14 [gpu_model_runner.py:1801] Model loading took 7.4885 GiB and 1.350249 seconds\n",
      "INFO 07-23 13:41:18 [backends.py:508] Using cache directory: /home/ntang/.cache/vllm/torch_compile_cache/b0e86e241c/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 07-23 13:41:18 [backends.py:519] Dynamo bytecode transform time: 4.26 s\n",
      "INFO 07-23 13:41:22 [backends.py:155] Directly load the compiled graph(s) for shape None from the cache, took 2.969 s\n",
      "INFO 07-23 13:41:23 [monitor.py:34] torch.compile takes 4.26 s in total\n",
      "INFO 07-23 13:41:24 [gpu_worker.py:232] Available KV cache memory: 34.28 GiB\n",
      "INFO 07-23 13:41:24 [kv_cache_utils.py:716] GPU KV cache size: 99,840 tokens\n",
      "INFO 07-23 13:41:24 [kv_cache_utils.py:720] Maximum concurrency for 8,192 tokens per request: 12.19x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:17<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-23 13:41:41 [gpu_model_runner.py:2326] Graph capturing finished in 17 secs, took 0.47 GiB\n",
      "INFO 07-23 13:41:41 [core.py:172] init engine (profile, create kv cache, warmup model) took 27.30 seconds\n"
     ]
    }
   ],
   "source": [
    "sampler = VLLMSampler(\n",
    "    model_name=\"JetBrains/Mellum-4b-sft-python\",\n",
    "    special_tokens=mellum_tokens.special_tokens(),\n",
    "    sampling_parameters=sampling_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "s9ldZuGu6gSImsSYjnsoIV",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247/247 [00:01<00:00, 199.19it/s]\n",
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 247/247 [03:21<00:00,  1.23it/s, est. speed input: 6132.18 toks/s, output: 143.61 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mellum ChrF: 0.5395045573048932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mellum_completions = sampler.generate(mellum_prompts)\n",
    "\n",
    "chrf_values = [\n",
    "    chrf(answer[\"middle\"], prediction)\n",
    "    for answer, prediction in zip(answers, mellum_completions)\n",
    "]\n",
    "\n",
    "print(\"Mellum ChrF:\", sum(chrf_values) / len(chrf_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default_3_11",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [
    {
     "name": "vllm",
     "source": "PIP"
    },
    {
     "name": "jsonlines",
     "source": "PIP",
     "version": "4.0.0"
    },
    {
     "name": "codegen-metrics",
     "source": "PIP",
     "version": "0.2.0"
    }
   ],
   "report_row_ids": [],
   "report_tabs": [],
   "version": 4
  },
  "kernelspec": {
   "display_name": "jb-completion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00647efc098e49d5987b6415692a05d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.2.0",
       "description_width": ""
      }
     },
     "007e779bdf8c484d8e95150ec3355ded": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.2.0",
       "description_width": ""
      }
     },
     "02ce0c09106c4ba88a99118a0c58a116": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.5.0",
       "children": [
        "IPY_MODEL_eed164e3525b4125a4add76141e06e15",
        "IPY_MODEL_82332cb6484944189f7fe08ff047f615",
        "IPY_MODEL_71e27437956643179bebcd45769070a4"
       ],
       "layout": "IPY_MODEL_32d56f7e987e47ff86a9034332966728"
      }
     },
     "038411bc51a8499482c0f38640eecc38": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.5.0",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b32c494f0c2e4bcd960b0782cee0323a",
       "style": "IPY_MODEL_00647efc098e49d5987b6415692a05d2",
       "value": "â€‡47/47â€‡[00:32&lt;00:00,â€‡â€‡1.51it/s,â€‡est.â€‡speedâ€‡input:â€‡3015.62â€‡toks/s,â€‡output:â€‡183.84â€‡toks/s]"
      }
     },
     "07ad81a052a34ba7b71fcafeb133770b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.2.0",
       "description_width": ""
      }
     },
     "0df36e90e6454f86b709d1b555039cc0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.2.0",
       "description_width": ""
      }
     },
     "0e882a0b02694f2895a7ce8f98d26192": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "display": "inline-flex",
       "flex_flow": "row wrap",
       "width": "100%"
      }
     },
     "12838046fdb84e01bb91a51923a6ae72": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2dfcd905b06345ea9becc6e0714ad15b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.2.0",
       "description_width": ""
      }
     },
     "32d56f7e987e47ff86a9034332966728": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "410e181ac37a42c8a0c2b98a9fc71c08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.5.0",
       "bar_style": "success",
       "description_tooltip": null,
       "layout": "IPY_MODEL_af2b830f1e2840d6ab9319062ae8ef32",
       "max": 47,
       "style": "IPY_MODEL_0df36e90e6454f86b709d1b555039cc0",
       "value": 47
      }
     },
     "46609a6322324ebaa4c57acee76b2c07": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.5.0",
       "description_tooltip": null,
       "layout": "IPY_MODEL_aed6e62dd59d4ee38b224bc103d0f3a9",
       "style": "IPY_MODEL_2dfcd905b06345ea9becc6e0714ad15b"
      }
     },
     "491a6b784609486e810891a7f08ead21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.2.0",
       "description_width": ""
      }
     },
     "4e3ede97ef4d4d9f98ee233c30da785e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "53c61680a06f4fe1b9bde39042612397": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5b429c420968442abfebf9d164f9d5b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.5.0",
       "description_tooltip": null,
       "layout": "IPY_MODEL_78598ca940ba43b1a2575e1d029daa5f",
       "max": 47,
       "style": "IPY_MODEL_07ad81a052a34ba7b71fcafeb133770b"
      }
     },
     "69dada51ad254ef9a66adc8236633a48": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "71e27437956643179bebcd45769070a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.5.0",
       "description_tooltip": null,
       "layout": "IPY_MODEL_de0fb480f2bf467086a231bb157ad361",
       "style": "IPY_MODEL_b4b5eebbee5c4b748890d832e98bed40",
       "value": "Loadingâ€‡safetensorsâ€‡checkpointâ€‡shards:â€‡100%â€‡Completedâ€‡|â€‡2/2â€‡[00:05&lt;00:00,â€‡â€‡3.25s/it]\n"
      }
     },
     "78598ca940ba43b1a2575e1d029daa5f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "810d80b0fdc04109bf6c0a128200eff1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.2.0",
       "description_width": ""
      }
     },
     "82332cb6484944189f7fe08ff047f615": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.5.0",
       "bar_style": "success",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4e3ede97ef4d4d9f98ee233c30da785e",
       "max": 2,
       "style": "IPY_MODEL_491a6b784609486e810891a7f08ead21",
       "value": 2
      }
     },
     "876b8ea409ca4d3da92c666666d8cb61": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9db21d71147940a29078ad3b77478b4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.5.0",
       "children": [
        "IPY_MODEL_ac9b6e2493f0478893632faf773d98aa",
        "IPY_MODEL_410e181ac37a42c8a0c2b98a9fc71c08",
        "IPY_MODEL_038411bc51a8499482c0f38640eecc38"
       ],
       "layout": "IPY_MODEL_0e882a0b02694f2895a7ce8f98d26192"
      }
     },
     "ac9b6e2493f0478893632faf773d98aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.5.0",
       "description_tooltip": null,
       "layout": "IPY_MODEL_69dada51ad254ef9a66adc8236633a48",
       "style": "IPY_MODEL_810d80b0fdc04109bf6c0a128200eff1",
       "value": "Processedâ€‡prompts:â€‡100%"
      }
     },
     "aed6e62dd59d4ee38b224bc103d0f3a9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "af2b830f1e2840d6ab9319062ae8ef32": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "flex": "2"
      }
     },
     "b32c494f0c2e4bcd960b0782cee0323a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b3a1e2a4ad6f44cea74d8ddcb52401eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.2.0",
       "description_width": ""
      }
     },
     "b4b5eebbee5c4b748890d832e98bed40": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.2.0",
       "description_width": ""
      }
     },
     "de0fb480f2bf467086a231bb157ad361": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "df242061bf234a319c45bad4e2e5351c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.5.0",
       "description_tooltip": null,
       "layout": "IPY_MODEL_876b8ea409ca4d3da92c666666d8cb61",
       "style": "IPY_MODEL_b3a1e2a4ad6f44cea74d8ddcb52401eb",
       "value": "Adding requests"
      }
     },
     "eeb776c43be14636a725d392a2b3bc09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.5.0",
       "children": [
        "IPY_MODEL_df242061bf234a319c45bad4e2e5351c",
        "IPY_MODEL_5b429c420968442abfebf9d164f9d5b6",
        "IPY_MODEL_46609a6322324ebaa4c57acee76b2c07"
       ],
       "layout": "IPY_MODEL_12838046fdb84e01bb91a51923a6ae72"
      }
     },
     "eed164e3525b4125a4add76141e06e15": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_model_module_version": "1.5.0",
       "_view_module_version": "1.5.0",
       "description_tooltip": null,
       "layout": "IPY_MODEL_53c61680a06f4fe1b9bde39042612397",
       "style": "IPY_MODEL_007e779bdf8c484d8e95150ec3355ded",
       "value": "Loading safetensors checkpoint shards"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
